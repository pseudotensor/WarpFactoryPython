{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warp Drive Acceleration Results Visualization\n",
    "\n",
    "This notebook visualizes the results from all 6 acceleration approaches.\n",
    "\n",
    "**Date:** October 15, 2025\n",
    "\n",
    "**Simulation ID:** 20251015_080457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from acceleration_research.results_comparison import load_results, compare_all_approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results\n",
    "results_file = '../results/all_results_20251015_080457.pkl'\n",
    "comparison_file = '../results/comparison_20251015_080457.pkl'\n",
    "\n",
    "with open(results_file, 'rb') as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "with open(comparison_file, 'rb') as f:\n",
    "    comparison = pickle.load(f)\n",
    "\n",
    "print(\"Loaded results for\", len(all_results), \"approaches\")\n",
    "print(\"\\nApproaches:\")\n",
    "for i, name in enumerate(all_results.keys(), 1):\n",
    "    print(f\"{i}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rankings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL RANKINGS (Best to Worst)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rankings = comparison['overall_rankings']\n",
    "for rank, (approach, score) in enumerate(rankings, 1):\n",
    "    symbol = \"⭐\" if rank == 1 else \"❌\" if rank == len(rankings) else \"⚠️\"\n",
    "    print(f\"{rank}. {symbol} {approach}: {score:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violation Comparison Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of violations\n",
    "approaches = comparison['approaches']\n",
    "conditions = comparison['conditions']\n",
    "\n",
    "# Collect violation data\n",
    "violation_matrix = np.zeros((len(approaches), len(conditions)))\n",
    "for i, approach in enumerate(approaches):\n",
    "    for j, condition in enumerate(conditions):\n",
    "        if approach in comparison['metrics_by_condition'][condition]:\n",
    "            viol = comparison['metrics_by_condition'][condition][approach]['worst_violation']\n",
    "            # Use log10 of absolute value for visualization\n",
    "            violation_matrix[i, j] = np.log10(abs(viol)) if not np.isnan(viol) else 0\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(violation_matrix, aspect='auto', cmap='RdYlGn_r', interpolation='nearest')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(conditions)))\n",
    "ax.set_yticks(np.arange(len(approaches)))\n",
    "ax.set_xticklabels(conditions)\n",
    "ax.set_yticklabels([a.split('(')[0].strip() for a in approaches])\n",
    "\n",
    "# Rotate x labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel('log₁₀(|Violation|)', rotation=-90, va=\"bottom\")\n",
    "\n",
    "# Add values in cells\n",
    "for i in range(len(approaches)):\n",
    "    for j in range(len(conditions)):\n",
    "        text = ax.text(j, i, f\"{violation_matrix[i, j]:.1f}\",\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "ax.set_title(\"Energy Condition Violations by Approach\\n(log₁₀ scale, lower is better)\", pad=20)\n",
    "fig.tight_layout()\n",
    "plt.savefig('../results/figures/violation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Heatmap saved to results/figures/violation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized performance (relative to worst)\n",
    "scores = [score for _, score in rankings]\n",
    "names = [name.split('(')[0].strip() for name, _ in rankings]\n",
    "\n",
    "# Normalize to worst = 1\n",
    "worst_score = min(scores)\n",
    "normalized = [s / worst_score for s in scores]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(names)))\n",
    "bars = ax.barh(names, normalized, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, normalized)):\n",
    "    ax.text(val + 0.02, i, f\"{val:.2f}x\", va='center', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Performance (Normalized to Worst = 1.0, Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Approach Performance Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Baseline')\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Performance chart saved to results/figures/performance_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create detailed table\n",
    "data = []\n",
    "for approach in approaches:\n",
    "    row = {'Approach': approach.split('(')[0].strip()}\n",
    "    for condition in conditions:\n",
    "        if approach in comparison['metrics_by_condition'][condition]:\n",
    "            metrics = comparison['metrics_by_condition'][condition][approach]\n",
    "            row[f\"{condition}_Worst\"] = f\"{metrics['worst_violation']:.2e}\"\n",
    "            row[f\"{condition}_Frac%\"] = f\"{metrics['fraction_violating']*100:.1f}\"\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"DETAILED METRICS TABLE\")\n",
    "print(\"=\"*120)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('../results/detailed_metrics.csv', index=False)\n",
    "print(\"\\n✓ Table saved to results/detailed_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winner Analysis: Multi-Shell Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the winner\n",
    "winner_name = rankings[0][0]\n",
    "winner_score = rankings[0][1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WINNER ANALYSIS: MULTI-SHELL CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nOverall Score: {winner_score:.6e}\")\n",
    "print(f\"\\nPerformance by Energy Condition:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for condition in conditions:\n",
    "    metrics = comparison['metrics_by_condition'][condition][winner_name]\n",
    "    print(f\"\\n{condition} Energy Condition:\")\n",
    "    print(f\"  Worst Violation: {metrics['worst_violation']:.6e}\")\n",
    "    print(f\"  Max Magnitude:   {metrics['max_magnitude']:.6e}\")\n",
    "    print(f\"  L2 Norm:         {metrics['total_L2']:.6e}\")\n",
    "    print(f\"  Fraction Violating: {metrics['fraction_violating']:.1%}\")\n",
    "\n",
    "# Compare to worst\n",
    "worst_name = rankings[-1][0]\n",
    "worst_score = rankings[-1][1]\n",
    "improvement = worst_score / winner_score\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"IMPROVEMENT OVER BASELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Worst Performer: {worst_name.split('(')[0].strip()}\")\n",
    "print(f\"Improvement Factor: {improvement:.1f}x better\")\n",
    "print(f\"\\n⭐ Multi-Shell reduces violations by ~{improvement:.0f}x compared to naive approach!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESEARCH SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nApproaches Tested: {len(approaches)}\")\n",
    "print(f\"Energy Conditions Evaluated: {len(conditions)}\")\n",
    "print(f\"Simulation Date: {comparison['timestamp']}\")\n",
    "\n",
    "print(f\"\\nBest Approach: {winner_name.split('(')[0].strip()} ⭐\")\n",
    "print(f\"Worst Approach: {worst_name.split('(')[0].strip()} ❌\")\n",
    "print(f\"Improvement Factor: {improvement:.1f}x\")\n",
    "\n",
    "print(f\"\\nViolation Severity:\")\n",
    "for condition in conditions:\n",
    "    all_viols = [comparison['metrics_by_condition'][condition][a]['worst_violation'] \n",
    "                 for a in approaches if a in comparison['metrics_by_condition'][condition]]\n",
    "    print(f\"  {condition}: {min(all_viols):.2e} to {max(all_viols):.2e}\")\n",
    "\n",
    "print(f\"\\n✅ All 6 approaches successfully simulated and analyzed\")\n",
    "print(f\"✅ Clear winner identified with significant improvement\")\n",
    "print(f\"❌ No complete solution found (all violate energy conditions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Multi-Shell Configuration is the clear winner** - 59x better than baseline\n",
    "2. **All approaches violate energy conditions** - 100% of spacetime, 100% of time\n",
    "3. **Gradual transition (naive) performs worst** - Confirms acceleration is fundamentally difficult\n",
    "4. **Modified lapse functions show promise** - Second-best performer\n",
    "\n",
    "### Physical Interpretation:\n",
    "\n",
    "The multi-shell \"velocity ladder\" approach reduces violations by smoothing metric gradients across space. This represents the most promising direction for future acceleration research.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Optimize multi-shell parameters (number of shells, velocity ratios, mass distribution)\n",
    "2. Run full-resolution simulations\n",
    "3. Test combined approaches (multi-shell + modified lapse)\n",
    "4. Explore quantum corrections and modified gravity\n",
    "\n",
    "---\n",
    "\n",
    "**Report Generated:** October 15, 2025\n",
    "\n",
    "**Simulation ID:** 20251015_080457\n",
    "\n",
    "**Status:** Complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
