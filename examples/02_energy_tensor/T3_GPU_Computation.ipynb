{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Energy Tensor Computation\n",
    "\n",
    "This notebook demonstrates how to use GPU acceleration with WarpFactory to significantly speed up stress-energy tensor calculations. GPU computation is particularly beneficial for:\n",
    "\n",
    "- Large grid sizes (>100x100x100)\n",
    "- Batch processing of multiple metrics\n",
    "- High-resolution simulations\n",
    "- Real-time visualization and analysis\n",
    "\n",
    "WarpFactory uses [CuPy](https://cupy.dev/) for GPU acceleration, which provides a NumPy-compatible interface that runs on NVIDIA CUDA GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## GPU Availability Check\n",
    "\n",
    "First, let's check if GPU acceleration is available on this system. The code will gracefully fall back to CPU computation if CuPy is not installed or if no GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(\"CuPy version:\", cp.__version__)\n",
    "    print(\"CUDA available:\", cp.cuda.is_available())\n",
    "    if cp.cuda.is_available():\n",
    "        print(\"GPU device:\", cp.cuda.Device().name)\n",
    "        print(\"GPU memory:\", cp.cuda.Device().mem_info[1] / 1e9, \"GB\")\n",
    "    GPU_AVAILABLE = cp.cuda.is_available()\n",
    "except ImportError:\n",
    "    print(\"CuPy is not installed. GPU acceleration will not be available.\")\n",
    "    print(\"To install CuPy, visit: https://docs.cupy.dev/en/stable/install.html\")\n",
    "    GPU_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WarpFactory modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from warpfactory.metrics.alcubierre import get_alcubierre_metric\n",
    "from warpfactory.solver.energy import get_energy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Method 1: Using the try_gpu Parameter\n",
    "\n",
    "The simplest way to use GPU acceleration is with the `try_gpu=True` parameter in `get_energy_tensor()`. This automatically:\n",
    "1. Transfers the metric to GPU memory\n",
    "2. Performs all computations on the GPU\n",
    "3. Transfers results back to CPU memory\n",
    "\n",
    "If CuPy is not available, it automatically falls back to CPU computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a moderate-size Alcubierre metric for testing\n",
    "grid_size = [1, 50, 50, 50]\n",
    "world_center = [(grid_size[i] + 1) / 2 for i in range(4)]\n",
    "\n",
    "print(\"Creating Alcubierre metric...\")\n",
    "metric = get_alcubierre_metric(\n",
    "    grid_size=grid_size,\n",
    "    world_center=world_center,\n",
    "    velocity=0.9,\n",
    "    radius=10,\n",
    "    sigma=0.5\n",
    ")\n",
    "\n",
    "print(f\"Metric created: {metric.name}\")\n",
    "print(f\"Grid shape: {metric.shape}\")\n",
    "print(f\"Total grid points: {np.prod(metric.shape):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU computation\n",
    "print(\"Computing energy tensor on CPU...\")\n",
    "start_cpu = time.time()\n",
    "energy_cpu = get_energy_tensor(metric, try_gpu=False)\n",
    "time_cpu = time.time() - start_cpu\n",
    "print(f\"CPU time: {time_cpu:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU computation (if available)\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"Computing energy tensor on GPU...\")\n",
    "    start_gpu = time.time()\n",
    "    energy_gpu = get_energy_tensor(metric, try_gpu=True)\n",
    "    time_gpu = time.time() - start_gpu\n",
    "    print(f\"GPU time: {time_gpu:.3f} seconds\")\n",
    "    print(f\"Speedup: {time_cpu/time_gpu:.2f}x\")\n",
    "    \n",
    "    # Verify results match\n",
    "    max_diff = np.max(np.abs(energy_cpu.tensor[(0,0)] - energy_gpu.tensor[(0,0)]))\n",
    "    print(f\"Maximum difference between CPU and GPU results: {max_diff:.2e}\")\n",
    "else:\n",
    "    print(\"\\nGPU not available - skipping GPU computation\")\n",
    "    print(\"The try_gpu=True parameter will automatically fall back to CPU\")\n",
    "    energy_safe = get_energy_tensor(metric, try_gpu=True)\n",
    "    print(\"Computation completed successfully on CPU (fallback)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Method 2: Manual GPU Transfer with Tensor Methods\n",
    "\n",
    "For more control, you can manually transfer tensors to/from GPU memory using:\n",
    "- `.to_gpu()`: Transfer tensor from CPU to GPU\n",
    "- `.to_cpu()`: Transfer tensor from GPU to CPU\n",
    "- `.is_gpu()`: Check if tensor is on GPU\n",
    "\n",
    "This is useful when:\n",
    "- You want to keep data on GPU for multiple operations\n",
    "- You need to minimize data transfer overhead\n",
    "- You're working with custom analysis pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate tensor GPU methods\n",
    "print(\"Metric is on GPU:\", metric.is_gpu())\n",
    "print(\"Energy tensor is on GPU:\", energy_cpu.is_gpu())\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    # Transfer metric to GPU\n",
    "    print(\"\\nTransferring metric to GPU...\")\n",
    "    metric_gpu = metric.to_gpu()\n",
    "    print(\"Metric is now on GPU:\", metric_gpu.is_gpu())\n",
    "    \n",
    "    # Check memory usage\n",
    "    import cupy as cp\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    print(f\"GPU memory used: {mempool.used_bytes() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Transfer back to CPU\n",
    "    print(\"\\nTransferring back to CPU...\")\n",
    "    metric_cpu_again = metric_gpu.to_cpu()\n",
    "    print(\"Metric is on GPU:\", metric_cpu_again.is_gpu())\n",
    "    \n",
    "    # Free GPU memory\n",
    "    del metric_gpu\n",
    "    mempool.free_all_blocks()\n",
    "    print(f\"GPU memory freed: {mempool.used_bytes() / 1e6:.1f} MB\")\n",
    "else:\n",
    "    print(\"\\nGPU methods work safely without CuPy:\")\n",
    "    print(\"- to_gpu() will raise ImportError\")\n",
    "    print(\"- to_cpu() returns a copy if not on GPU\")\n",
    "    print(\"- is_gpu() returns False\")\n",
    "    \n",
    "    # This is safe - returns a copy\n",
    "    metric_copy = metric.to_cpu()\n",
    "    print(\"\\nto_cpu() created a safe copy on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Performance Comparison: CPU vs GPU\n",
    "\n",
    "Let's compare performance across different grid sizes to see where GPU acceleration provides the most benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison across different grid sizes\n",
    "if GPU_AVAILABLE:\n",
    "    grid_sizes = [\n",
    "        [1, 20, 20, 20],\n",
    "        [1, 40, 40, 40],\n",
    "        [1, 60, 60, 60],\n",
    "    ]\n",
    "    \n",
    "    cpu_times = []\n",
    "    gpu_times = []\n",
    "    grid_points = []\n",
    "    \n",
    "    for grid_size in grid_sizes:\n",
    "        print(f\"\\nTesting grid size: {grid_size}\")\n",
    "        world_center = [(grid_size[i] + 1) / 2 for i in range(4)]\n",
    "        \n",
    "        # Create metric\n",
    "        test_metric = get_alcubierre_metric(\n",
    "            grid_size=grid_size,\n",
    "            world_center=world_center,\n",
    "            velocity=0.9,\n",
    "            radius=5,\n",
    "            sigma=0.5\n",
    "        )\n",
    "        \n",
    "        points = np.prod(grid_size)\n",
    "        grid_points.append(points)\n",
    "        \n",
    "        # CPU timing\n",
    "        start = time.time()\n",
    "        _ = get_energy_tensor(test_metric, try_gpu=False)\n",
    "        cpu_time = time.time() - start\n",
    "        cpu_times.append(cpu_time)\n",
    "        print(f\"  CPU: {cpu_time:.3f}s\")\n",
    "        \n",
    "        # GPU timing\n",
    "        start = time.time()\n",
    "        _ = get_energy_tensor(test_metric, try_gpu=True)\n",
    "        gpu_time = time.time() - start\n",
    "        gpu_times.append(gpu_time)\n",
    "        print(f\"  GPU: {gpu_time:.3f}s (speedup: {cpu_time/gpu_time:.2f}x)\")\n",
    "        \n",
    "        # Clean up\n",
    "        del test_metric\n",
    "        import cupy as cp\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Computation time\n",
    "    ax1.plot(grid_points, cpu_times, 'o-', label='CPU', linewidth=2)\n",
    "    ax1.plot(grid_points, gpu_times, 's-', label='GPU', linewidth=2)\n",
    "    ax1.set_xlabel('Total Grid Points')\n",
    "    ax1.set_ylabel('Computation Time (seconds)')\n",
    "    ax1.set_title('CPU vs GPU Performance')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Speedup factor\n",
    "    speedups = [cpu_times[i]/gpu_times[i] for i in range(len(cpu_times))]\n",
    "    ax2.plot(grid_points, speedups, 'o-', linewidth=2, color='green')\n",
    "    ax2.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Break-even')\n",
    "    ax2.set_xlabel('Total Grid Points')\n",
    "    ax2.set_ylabel('Speedup Factor (CPU time / GPU time)')\n",
    "    ax2.set_title('GPU Speedup Factor')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAverage speedup: {np.mean(speedups):.2f}x\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping performance comparison\")\n",
    "    print(\"\\nTypical GPU speedups on NVIDIA GPUs:\")\n",
    "    print(\"  Small grids (20^3):  1-2x\")\n",
    "    print(\"  Medium grids (50^3): 3-5x\")\n",
    "    print(\"  Large grids (100^3): 5-10x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Memory Benefits of GPU Computation\n",
    "\n",
    "GPU computation can also help with memory management:\n",
    "- GPU memory is separate from system RAM\n",
    "- Large grids can be processed without exhausting system memory\n",
    "- Multiple metrics can be batched on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    import cupy as cp\n",
    "    \n",
    "    # Create a moderately large metric\n",
    "    grid_size = [1, 50, 50, 50]\n",
    "    world_center = [(grid_size[i] + 1) / 2 for i in range(4)]\n",
    "    \n",
    "    print(\"Creating metric...\")\n",
    "    metric = get_alcubierre_metric(\n",
    "        grid_size=grid_size,\n",
    "        world_center=world_center,\n",
    "        velocity=0.9,\n",
    "        radius=10,\n",
    "        sigma=0.5\n",
    "    )\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    bytes_per_component = metric.tensor[(0,0)].nbytes\n",
    "    total_metric_memory = bytes_per_component * 16  # 4x4 tensor\n",
    "    \n",
    "    print(f\"\\nCPU Memory Usage:\")\n",
    "    print(f\"  Per component: {bytes_per_component / 1e6:.2f} MB\")\n",
    "    print(f\"  Full metric (16 components): {total_metric_memory / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Transfer to GPU and monitor memory\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    mempool.free_all_blocks()\n",
    "    \n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"  Before transfer: {mempool.used_bytes() / 1e6:.2f} MB\")\n",
    "    \n",
    "    metric_gpu = metric.to_gpu()\n",
    "    print(f\"  After metric transfer: {mempool.used_bytes() / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Compute energy tensor on GPU\n",
    "    energy_gpu = get_energy_tensor(metric, try_gpu=True)\n",
    "    print(f\"  After energy computation: {mempool.used_bytes() / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Clean up\n",
    "    del metric_gpu\n",
    "    mempool.free_all_blocks()\n",
    "    print(f\"  After cleanup: {mempool.used_bytes() / 1e6:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nNote: GPU computation keeps intermediate results in GPU memory,\")\n",
    "    print(\"reducing CPU memory pressure for large-scale simulations.\")\n",
    "else:\n",
    "    print(\"GPU not available - memory management example\")\n",
    "    print(\"\\nWith GPU acceleration, memory usage is distributed:\")\n",
    "    print(\"  - Input metric and parameters stay in system RAM\")\n",
    "    print(\"  - Intermediate calculations happen in GPU memory\")\n",
    "    print(\"  - Final results are transferred back to system RAM\")\n",
    "    print(\"\\nThis allows processing grids larger than available system RAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "Let's visualize the energy tensor computed on GPU (or CPU if GPU not available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean metric for visualization\n",
    "grid_size = [1, 40, 40, 40]\n",
    "world_center = [(grid_size[i] + 1) / 2 for i in range(4)]\n",
    "\n",
    "metric = get_alcubierre_metric(\n",
    "    grid_size=grid_size,\n",
    "    world_center=world_center,\n",
    "    velocity=0.9,\n",
    "    radius=10,\n",
    "    sigma=0.5\n",
    ")\n",
    "\n",
    "# Compute with GPU if available\n",
    "energy = get_energy_tensor(metric, try_gpu=GPU_AVAILABLE)\n",
    "\n",
    "computation_type = \"GPU\" if GPU_AVAILABLE else \"CPU\"\n",
    "print(f\"Energy tensor computed on {computation_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key energy tensor components\n",
    "t_slice = 0\n",
    "z_slice = int(world_center[3])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "computation_type = \"GPU\" if GPU_AVAILABLE else \"CPU\"\n",
    "fig.suptitle(f'Energy Tensor Components ({computation_type} Computation)', fontsize=16)\n",
    "\n",
    "# Energy density T^00\n",
    "ax = axes[0, 0]\n",
    "data = energy.tensor[(0, 0)][t_slice, :, :, z_slice]\n",
    "im = ax.imshow(data.T, origin='lower', cmap='RdBu_r', aspect='auto')\n",
    "ax.set_title(r'Energy Density $T^{00}$', fontsize=14)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Momentum density T^01\n",
    "ax = axes[0, 1]\n",
    "data = energy.tensor[(0, 1)][t_slice, :, :, z_slice]\n",
    "im = ax.imshow(data.T, origin='lower', cmap='RdBu_r', aspect='auto')\n",
    "ax.set_title(r'x-Momentum Density $T^{01}$', fontsize=14)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Stress component T^11\n",
    "ax = axes[1, 0]\n",
    "data = energy.tensor[(1, 1)][t_slice, :, :, z_slice]\n",
    "im = ax.imshow(data.T, origin='lower', cmap='RdBu_r', aspect='auto')\n",
    "ax.set_title(r'xx-Stress $T^{11}$', fontsize=14)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Stress component T^22\n",
    "ax = axes[1, 1]\n",
    "data = energy.tensor[(2, 2)][t_slice, :, :, z_slice]\n",
    "im = ax.imshow(data.T, origin='lower', cmap='RdBu_r', aspect='auto')\n",
    "ax.set_title(r'yy-Stress $T^{22}$', fontsize=14)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy density statistics and cross-section\n",
    "energy_density = energy.tensor[(0, 0)][t_slice, :, :, z_slice]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 2D energy density\n",
    "ax = axes[0]\n",
    "im = ax.imshow(energy_density.T, origin='lower', cmap='RdBu_r', aspect='auto')\n",
    "ax.set_title(r'Energy Density $T^{00}$', fontsize=14)\n",
    "ax.set_xlabel('x index')\n",
    "ax.set_ylabel('y index')\n",
    "plt.colorbar(im, ax=ax, label='Energy Density')\n",
    "\n",
    "# Cross-section through center\n",
    "ax = axes[1]\n",
    "y_center = int(world_center[2])\n",
    "ax.plot(energy_density[:, y_center], linewidth=2)\n",
    "ax.set_title('Energy Density Cross-Section at y=center', fontsize=14)\n",
    "ax.set_xlabel('x index')\n",
    "ax.set_ylabel(r'$T^{00}$')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEnergy Density Statistics:\")\n",
    "print(f\"  Minimum: {np.min(energy_density):.6e}\")\n",
    "print(f\"  Maximum: {np.max(energy_density):.6e}\")\n",
    "print(f\"  Mean:    {np.mean(energy_density):.6e}\")\n",
    "print(f\"  Std Dev: {np.std(energy_density):.6e}\")\n",
    "print(f\"\\nNote: Negative energy density indicates exotic matter requirement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Best Practices for GPU Computation\n",
    "\n",
    "### When to Use GPU Acceleration:\n",
    "\n",
    "1. **Large grids** (>50x50x50 points): GPU overhead is amortized\n",
    "2. **Repeated computations**: Keep data on GPU between operations\n",
    "3. **Batch processing**: Process multiple metrics sequentially on GPU\n",
    "4. **Memory constraints**: Offload computation from system RAM to GPU memory\n",
    "\n",
    "### When to Use CPU:\n",
    "\n",
    "1. **Small grids** (<20x20x20 points): CPU is faster due to GPU transfer overhead\n",
    "2. **Single computations**: If you only need one result\n",
    "3. **No GPU available**: Code works seamlessly on CPU\n",
    "\n",
    "### Memory Management Tips:\n",
    "\n",
    "```python\n",
    "# Good: Use try_gpu for automatic management\n",
    "energy = get_energy_tensor(metric, try_gpu=True)\n",
    "\n",
    "# Good: Manual control for batch processing\n",
    "metric_gpu = metric.to_gpu()\n",
    "# ... perform multiple operations ...\n",
    "result_cpu = result_gpu.to_cpu()\n",
    "\n",
    "# Good: Clean up GPU memory when done\n",
    "import cupy as cp\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **GPU Availability Check**: How to check if CuPy and CUDA are available\n",
    "\n",
    "2. **Simple GPU Usage**: Using `try_gpu=True` for automatic GPU acceleration with CPU fallback\n",
    "\n",
    "3. **Manual GPU Control**: Using `.to_gpu()`, `.to_cpu()`, and `.is_gpu()` methods for fine-grained control\n",
    "\n",
    "4. **Performance Comparison**: Measuring CPU vs GPU computation times across different grid sizes\n",
    "\n",
    "5. **Memory Management**: Understanding GPU memory usage and benefits for large-scale computations\n",
    "\n",
    "6. **Best Practices**: Guidelines for when to use GPU acceleration and how to manage memory efficiently\n",
    "\n",
    "GPU acceleration can provide significant speedups (3-10x) for large-scale warp drive simulations, enabling exploration of higher-resolution grids and more complex metrics that would be impractical on CPU alone.\n",
    "\n",
    "### Installation Notes:\n",
    "\n",
    "To use GPU acceleration, install CuPy:\n",
    "\n",
    "```bash\n",
    "# For CUDA 11.x\n",
    "pip install cupy-cuda11x\n",
    "\n",
    "# For CUDA 12.x\n",
    "pip install cupy-cuda12x\n",
    "\n",
    "# See https://docs.cupy.dev/en/stable/install.html for details\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
